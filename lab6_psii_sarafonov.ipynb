{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Практическая работа 6: Работа с текстом. Токенизация, стемминг, лемматизация\n",
        "\n",
        "## Цель\n",
        "Изучить основные методы обработки текстовых данных, такие как токенизация, стемминг и лемматизация, и их применение для предобработки текстов перед анализом.\n",
        "\n",
        "### Шаги выполнения\n",
        "1. Изучение методов обработки текстов.\n",
        "2. Программирование токенизации текста.\n",
        "3. Программирование стемминга.\n",
        "4. Программирование лемматизации.\n",
        "\n",
        "\n",
        "## Дополнительные задания.\n",
        "### Библиотека\n",
        "NLTK\n",
        "\n",
        "## Исходные данные\n",
        "* Пример текста и слов для демонстрации методов предобработки.\n",
        "* Изучение методов обработки текстов\n",
        "* Токенизация, стемминг и лемматизация являются ключевыми методами предобработки текстовых данных:\n",
        "\n",
        "### Токенизация: процесс разделения текста на отдельные слова или фразы (токены).\n",
        "### Стемминг: процесс нахождения основных форм слов (стемов), часто путём удаления суффиксов.\n",
        "### Лемматизация: процесс преобразования слов к их базовой (лемматизированной) форме с учётом контекста.\n",
        "### Программирование токенизации текста"
      ],
      "metadata": {
        "id": "6V_shmPo1rty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Загрузка необходимых ресурсов NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Пример текста\n",
        "text = \"The quick brown fox jumps over the lazy dog. The dog was not happy about it.\"\n",
        "\n",
        "# Токенизация текста\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Инициализация стеммера\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Стемминг слов\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
        "\n",
        "# Инициализация лемматизатора\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Лемматизация слов\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pH_J_5BP2HX8",
        "outputId": "37256a70-9721-4110-a6e2-6c4af90d7f33"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'The', 'dog', 'was', 'not', 'happy', 'about', 'it', '.']\n",
            "Stemmed Tokens: ['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog', '.', 'the', 'dog', 'wa', 'not', 'happi', 'about', 'it', '.']\n",
            "Lemmatized Tokens: ['The', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazy', 'dog', '.', 'The', 'dog', 'wa', 'not', 'happy', 'about', 'it', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Дополнительное задание: Сравнение различных библиотек"
      ],
      "metadata": {
        "id": "cSQX1F9T2J9a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysGpooLdz4uZ",
        "outputId": "04bd4db1-c453-447f-fa77-3cf1666ac368"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Tokens: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'The', 'dog', 'was', 'not', 'happy', 'about', 'it', '.']\n",
            "SpaCy Tokens: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'The', 'dog', 'was', 'not', 'happy', 'about', 'it', '.']\n",
            "TextBlob Tokens: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'The', 'dog', 'was', 'not', 'happy', 'about', 'it']\n",
            "SpaCy Lemmatized Tokens: ['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazy', 'dog', '.', 'the', 'dog', 'be', 'not', 'happy', 'about', 'it', '.']\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Загрузка SpaCy модели\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Пример текста\n",
        "text = \"The quick brown fox jumps over the lazy dog. The dog was not happy about it.\"\n",
        "\n",
        "# NLTK токенизация\n",
        "nltk_tokens = word_tokenize(text)\n",
        "print(\"NLTK Tokens:\", nltk_tokens)\n",
        "\n",
        "# SpaCy токенизация\n",
        "spacy_doc = nlp(text)\n",
        "spacy_tokens = [token.text for token in spacy_doc]\n",
        "print(\"SpaCy Tokens:\", spacy_tokens)\n",
        "\n",
        "# TextBlob токенизация\n",
        "blob = TextBlob(text)\n",
        "textblob_tokens = blob.words\n",
        "print(\"TextBlob Tokens:\", textblob_tokens)\n",
        "\n",
        "# SpaCy лемматизация\n",
        "spacy_lemmas = [token.lemma_ for token in spacy_doc]\n",
        "print(\"SpaCy Lemmatized Tokens:\", spacy_lemmas)\n"
      ]
    }
  ]
}