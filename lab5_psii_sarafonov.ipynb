{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Практическая работа 5: Работа с текстом. Мешок слов\n",
        "## Цель\n",
        "Изучить основы работы с текстовыми данными и создание мешка слов для представления текстов в виде числовых признаков.\n",
        "\n",
        "## Шаги выполнения\n",
        "1. Изучение предобработки текстовых данных.\n",
        "2. Программирование создания мешка слов.\n",
        "3. Использование мешка слов в модели машинного обучения.\n",
        "\n",
        "\n",
        "## Дополнительные задания.\n",
        "### Библиотека\n",
        "scikit-learn\n",
        "\n",
        "## Исходные данные\n",
        "* Пример текстовых данных для обучения и тестирования модели.\n",
        "* Изучение предобработки текстовых данных\n",
        "* Предобработка текстовых данных включает:\n",
        "\n",
        "### Необходимо реализовать\n",
        "* Токенизацию: разделение текста на отдельные слова (токены).\n",
        "* Удаление стоп-слов: исключение из текста часто встречающихся слов, не несущих смысловой нагрузки (например, \"и\", \"в\", \"на\").\n",
        "Приведение к нижнему регистру: унификация текста для уменьшения количества различных токенов.\n",
        "* Программирование создания мешка слов"
      ],
      "metadata": {
        "id": "4eDhFXxZwDRR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iD11nhfjjG7-",
        "outputId": "4eeca0fe-de63-46d7-a1c4-f1116fd03615"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
            "Bag of Words Matrix:\n",
            "[[0 1 1 1 0 0 1 0 1]\n",
            " [0 2 0 1 0 1 1 0 1]\n",
            " [1 0 0 1 1 0 1 1 1]\n",
            " [0 1 1 1 0 0 1 0 1]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Пример текстовых данных\n",
        "documents = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\",\n",
        "]\n",
        "\n",
        "# Инициализация счетчика слов\n",
        "count_vectorizer = CountVectorizer()\n",
        "\n",
        "# Преобразование текста в мешок слов\n",
        "bag_of_words = count_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Вывод словаря (токенов)\n",
        "print(\"Vocabulary:\", count_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Вывод матрицы мешка слов\n",
        "print(\"Bag of Words Matrix:\")\n",
        "print(bag_of_words.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Использование мешка слов в модели машинного обучения"
      ],
      "metadata": {
        "id": "imXDHQiTwn2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Пример текстовых данных и их меток\n",
        "texts = [\"This is good\", \"This is bad\", \"This is terrible\", \"This is awesome\"]\n",
        "labels = [\"positive\", \"negative\", \"negative\", \"positive\"]\n",
        "\n",
        "# Разделение данных на обучающий и тестовый наборы\n",
        "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Преобразование текстов в мешок слов\n",
        "count_vectorizer.fit(texts)  # Нужно подогнать векторизатор на всём наборе данных\n",
        "X_train_bag_of_words = count_vectorizer.transform(X_train)\n",
        "X_test_bag_of_words = count_vectorizer.transform(X_test)\n",
        "\n",
        "# Инициализация и обучение модели\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_bag_of_words, y_train)\n",
        "\n",
        "# Оценка производительности модели\n",
        "accuracy = model.score(X_test_bag_of_words, y_test)\n",
        "print(\"Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdGO-6xzwmMH",
        "outputId": "ab930b66-7739-450f-aadc-20862287fe8c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Дополнительные задания\n",
        "Исследование влияния различных параметров создания мешка слов:\n",
        "Изучите влияние минимальной и максимальной частоты слов, использование n-грамм на производительность модели."
      ],
      "metadata": {
        "id": "DuJiB9yQwxGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример использования параметров минимальной и максимальной частоты слов и n-грамм\n",
        "count_vectorizer = CountVectorizer(min_df=1, max_df=0.75, ngram_range=(1, 2))\n",
        "\n",
        "# Преобразование текста в мешок слов с новыми параметрами\n",
        "count_vectorizer.fit(documents)\n",
        "bag_of_words = count_vectorizer.transform(documents)\n",
        "\n",
        "# Вывод словаря (токенов) и матрицы мешка слов с новыми параметрами\n",
        "print(\"Vocabulary with n-grams:\", count_vectorizer.get_feature_names_out())\n",
        "print(\"Bag of Words Matrix with n-grams:\")\n",
        "print(bag_of_words.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_i4_YOkwvaT",
        "outputId": "06e87c6d-37d1-4705-e1a2-8239836c0ac0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary with n-grams: ['and' 'and this' 'document' 'document is' 'first' 'first document'\n",
            " 'is the' 'is this' 'one' 'second' 'second document' 'the first'\n",
            " 'the second' 'the third' 'third' 'third one' 'this document' 'this is'\n",
            " 'this the']\n",
            "Bag of Words Matrix with n-grams:\n",
            "[[0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 1 0]\n",
            " [0 0 2 1 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0]\n",
            " [1 1 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0]\n",
            " [0 0 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Реализация альтернативных методов представления текста, таких как TF-IDF:"
      ],
      "metadata": {
        "id": "_7hMh9O4w3Re"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Инициализация TF-IDF векторизатора\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Преобразование текста в TF-IDF представление\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Вывод словаря (токенов)\n",
        "print(\"TF-IDF Vocabulary:\", tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Вывод матрицы TF-IDF\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(tfidf_matrix.toarray())\n",
        "\n",
        "# Применение TF-IDF к модели машинного обучения\n",
        "tfidf_vectorizer.fit(texts)  # Нужно подогнать векторизатор на всём наборе данных\n",
        "X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Инициализация и обучение модели\n",
        "model_tfidf = MultinomialNB()\n",
        "model_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Оценка производительности модели\n",
        "accuracy_tfidf = model_tfidf.score(X_test_tfidf, y_test)\n",
        "print(\"Model Accuracy with TF-IDF:\", accuracy_tfidf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0VsdMoYwzjT",
        "outputId": "b83e7e96-663a-4925-c6e1-0f7f099ec12e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Vocabulary: ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
            "TF-IDF Matrix:\n",
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]\n",
            " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
            "  0.28108867 0.         0.28108867]\n",
            " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
            "  0.26710379 0.51184851 0.26710379]\n",
            " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n",
            "Model Accuracy with TF-IDF: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Заключение\n",
        "В данной практической работе были изучены основы работы с текстовыми данными и создание мешка слов для представления текстов в виде числовых признаков. Были реализованы этапы предобработки текста, создание мешка слов, а также его применение в модели машинного обучения. Дополнительно были исследованы различные параметры создания мешка слов и реализация альтернативного метода представления текста (TF-IDF)."
      ],
      "metadata": {
        "id": "yHnub23exM9q"
      }
    }
  ]
}